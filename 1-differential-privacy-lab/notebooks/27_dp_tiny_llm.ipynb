{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Differential Privacy in Tiny LLM Training\n",
    "\n",
    "Built by **Stu** ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Basics of Private LLM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Define Private LLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_llm_training_definition = \"Training a language model while adding noise to gradients to protect sensitive text.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sketch Privacy Risks in LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_privacy_risks = \"Memorization of sensitive phrases (e.g., names, addresses) leading to privacy leaks in output.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Simulate Tiny Text Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create Tiny Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_corpus = [\n",
    "    \"I love Paris in the spring\",\n",
    "    \"New York is a vibrant city\",\n",
    "    \"Tokyo has amazing food\",\n",
    "    \"London is very rainy\",\n",
    "    \"Traveling expands your mind\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sentence in tiny_corpus:\n",
    "    vocab.update(sentence.lower().split())\n",
    "vocab = sorted(list(vocab))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Encode Sentences as Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = []\n",
    "for sentence in tiny_corpus:\n",
    "    encoded = [word2idx[word] for word in sentence.lower().split()]\n",
    "    encoded_sentences.append(encoded)\n",
    "encoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Train Toy LLM with DP Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Simulate Simple Embedding + Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "embedding_dim = 8\n",
    "W_embed = np.random.normal(0, 0.1, size=(len(vocab), embedding_dim))\n",
    "W_out = np.random.normal(0, 0.1, size=(embedding_dim, len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Add DP Noise to Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dp_noise(grads, epsilon=1.0):\n",
    "    return grads + np.random.laplace(0, 1/epsilon, size=grads.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Single Step Private Gradient Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example single (very simplified) training step\n",
    "x_idx = encoded_sentences[0][0]  # First word\n",
    "y_idx = encoded_sentences[0][1]  # Next word\n",
    "\n",
    "# Forward\n",
    "embedding = W_embed[x_idx]\n",
    "logits = embedding @ W_out\n",
    "\n",
    "# Loss Gradient (softmax cross-entropy simplified)\n",
    "probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "grad_logits = probs.copy()\n",
    "grad_logits[y_idx] -= 1\n",
    "\n",
    "# Backward\n",
    "grad_W_out = np.outer(embedding, grad_logits)\n",
    "grad_embed = grad_logits @ W_out.T\n",
    "\n",
    "# Add noise\n",
    "grad_W_out = add_dp_noise(grad_W_out, epsilon=1.0)\n",
    "grad_embed = add_dp_noise(grad_embed, epsilon=1.0)\n",
    "\n",
    "# Update\n",
    "W_out -= 0.1 * grad_W_out\n",
    "W_embed[x_idx] -= 0.1 * grad_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Reflect on Private Training Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_llm_reflection = \"DP noise on tiny datasets leads to unstable gradients; larger models need careful tuning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: Summarize Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_llm_summary = \"Private training prevents overfitting to rare phrases but slows convergence and hurts performance slightly.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}